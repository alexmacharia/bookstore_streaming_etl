{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b31af6-88e0-4b71-a03a-644161ad530c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0399fa2b-b044-490f-bef1-d70d42afdf15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter:\n",
    "    def __init__(self, merge_query, temp_view):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view = temp_view\n",
    "\n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        df_micro_batch.createOrReplaceTempView(self.temp_view)\n",
    "        df_micro_batch.sparkSession.sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abcd0bd4-e624-420f-964c-79889f533d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class CDCUpserter:\n",
    "    def __init__(self, merge_query, temp_view, id_col, sort_col):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view = temp_view\n",
    "        self.id_col = id_col\n",
    "        self.sort_col = sort_col\n",
    "\n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        window = Window.partitionBy(self.id_col).orderBy(F.col(self.sort_col).desc())\n",
    "\n",
    "        df_micro_batch.filter(F.col(\"row_status\").isin([\"insert\", \"update\"])) \\\n",
    "            .withColumn(\"rank\", F.rank().over(window)) \\\n",
    "            .filter(\"rank == 1\") \\\n",
    "            .drop(\"rank\") \\\n",
    "            .createOrReplaceTempView(self.temp_view)\n",
    "\n",
    "        df_micro_batch.sparkSession.sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b42295e-25f9-46f2-9c4a-33ed06a320e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CDFUpserter:\n",
    "    def __init__(self, merge_query, temp_view, sort_col, *id_cols):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view = temp_view\n",
    "        self.id_cols = id_cols\n",
    "        self.sort_col = sort_col\n",
    "\n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        window = Window.partition_by(*self.id_cols).orderBy(F.col(self.sort_col).desc())\n",
    "\n",
    "        df_micro_batch.filter(F.col(\"_change_type\").isin([\"insert\", \"update_postimage\"])) \\\n",
    "            .withColumn(\"rank\", F.rank().over(window)) \\\n",
    "            .filter(\"rank == 1\") \\\n",
    "            .drop(\"rank\", \"_change_type\", \"_commit_version\") \\\n",
    "            .withColumnRenamed(\"commit_timestamp\", \"processed_timestamp\") \\\n",
    "            .createOrReplaceTempView(self.temp_view)\n",
    "\n",
    "        df_micro_batch.sparkSession.sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993e8ed3-7a97-4973-8cf6-458e78e09890",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver class definition (fixed boolean)"
    }
   },
   "outputs": [],
   "source": [
    "class Silver:\n",
    "    def __init__(self, env):\n",
    "        self.Conf = Config()\n",
    "        self.checkpoint_dir = self.Conf.base_checkpoint_dir + \"/checkpoints\"\n",
    "        self.landing_dir = self.Conf.base_data_dir + \"/raw\"\n",
    "        self.catalog = env\n",
    "        self.db_name = self.Conf.db_name\n",
    "        self.maxFilesPerTrigger = self.Conf.maxFilesPerTrigger\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "\n",
    "    \n",
    "    def upsert_customers(self, once=True, processing_time=\"10 seconds\"):\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.silver_customers c\n",
    "            USING ranked_updates r\n",
    "            ON c.customer_id=r.customer_id\n",
    "            WHEN MATCHED AND c.row_time < r.row_time\n",
    "              THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED\n",
    "              THEN INSERT *        \n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = CDCUpserter(query, \"ranked_updates\", \"customer_id\", \"row_time\")\n",
    "\n",
    "        schema = \"\"\"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING,\n",
    "            country_code STRING, row_status STRING, row_time timestamp\"\"\"\n",
    "\n",
    "        df_country_lookup = spark.read.json(self.landing_dir + \"/country_lookup\")\n",
    "\n",
    "        df_stream = (spark.readStream\n",
    "                          .table(f\"{self.catalog}.{self.db_name}.bronze\")\n",
    "                          .filter(\"topic = 'customers'\")\n",
    "                          .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"c\"))\n",
    "                          .select(\"c.*\")\n",
    "                          .join(F.broadcast(df_country_lookup), F.col(\"country_code\") == F.col(\"code\"))\n",
    "                    )\n",
    "\n",
    "        stream_writer = (df_stream.writeStream\n",
    "                                  .foreachBatch(data_upserter.upsert)\n",
    "                                  .option(\"checkpointLocation\", self.checkpoint_dir + \"/silver_customers\")                 \n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "    \n",
    "    def upsert_orders(self, once=True, processing_time=\"10 seconds\"):\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.silver_orders o\n",
    "            USING orders_temp t\n",
    "            ON o.order_id=t.order_id AND o.order_timestamp=t.order_timestamp\n",
    "            WHEN NOT MATCHED THEN INSERT *        \n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"orders_temp\")\n",
    "\n",
    "        schema = \"\"\"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT,\n",
    "            books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\"\"\n",
    "        \n",
    "        df_stream = (spark.readStream\n",
    "                          .table(f\"{self.catalog}.{self.db_name}.bronze\")\n",
    "                          .filter(\"topic='orders'\")\n",
    "                          .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"o\"))\n",
    "                          .select(\"o.*\")\n",
    "                          .dropDuplicates([\"order_id\", \"order_timestamp\"])\n",
    "                    )\n",
    "\n",
    "        stream_writer = (df_stream.writeStream\n",
    "                                  .foreachBatch(data_upserter.upsert)\n",
    "                                  .option(\"checkpointLocation\", self.checkpoint_dir + \"/silver_orders\")                 \n",
    "                        )\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).strart()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "    def upsert_books(self, once=True, processing_time=\"10 seconds\"):\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.silver_books\n",
    "            USING (\n",
    "                SELECT updates.book_id as merge_key, updates.*\n",
    "                FROM updates\n",
    "                UNION ALL\n",
    "                SELECT NULL as merge_key, updates.*\n",
    "                FROM updates\n",
    "                JOIN silver_books ON updates.book_id = silver_books.book_id\n",
    "                WHERE silver_books.current = true AND updates.price <> silver_books.price\n",
    "                ) staged_updates\n",
    "            ON silver_books.book_id = merge_key\n",
    "            WHEN MATCHED AND silver_books.current = true AND silver_books.price <> staged_updates.price THEN\n",
    "               UPDATE SET current = false, end_date = staged_updates.updated\n",
    "            WHEN NOT MATCHED THEN\n",
    "               INSERT (book_id, title, author, price, current, effective_date, end_date)\n",
    "               VALUES (staged_updates.book_id, staged_updates.title, staged_updates.author, staged_updates.price, true, staged_updates.updated, NULL)\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"updates\")\n",
    "\n",
    "        schema =  schema = \"book_id STRING, title STRING, author STRING, price DOUBLE, updated TIMESTAMP\"\n",
    "\n",
    "        df_stream = (spark.readStream\n",
    "                          .table(f\"{self.catalog}.{self.db_name}.silver_books\")\n",
    "                          .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"b\"))\n",
    "                          .select(\"b.*\")\n",
    "                    )\n",
    "        \n",
    "        stream_writer = (df_stream.writeStream\n",
    "                                  .foreachBatch(data_upserter.upsert)\n",
    "                                  .option(\"checkpointLocation\", self.checkpoint_dir + \"/silver_books\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            stream_writer.trigger(availableNow=True).strart()\n",
    "        else:\n",
    "            stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    \n",
    "    def upsert_books_sales(self, once=True, processing_time=\"10 seconds\"):\n",
    "        df_stream = (spark.readStream\n",
    "                          .table(f\"{self.catalog}.{self.db_name}.silver_orders\")\n",
    "                          .withColumn(\"book\", F.explode(\"books\"))\n",
    "                    )\n",
    "        current_books = spark.read.table(f\"{self.catalog}.{self.db_name}.current_books\")\n",
    "\n",
    "        stream_writer = (df_stream.join(current_books, df_stream.book.book_id ==current_books.book_id)\n",
    "                                  .writeStream\n",
    "                                  .option(\"checkpointLocation\", self.checkpoint_dir + \"/silver_book_sales\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            stream_writer.trigger(availableNow=True).strart()\n",
    "        else:\n",
    "            stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "    def upsert_customers_orders(self, once=True, processing_time=\"10 seconds\"):\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.silver_customers_orders co\n",
    "            USING updates u\n",
    "            ON co.order_id=u.order_id AND co.customer_id=u.customer_id\n",
    "            WHEN MATCHED AND co.processed_timestamp < u.processed_timestamp \n",
    "              THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED \n",
    "              THEN INSERT *        \n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = CDFUpserter(query, \"updates\")\n",
    "\n",
    "        schhema = \"\"\"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING,\n",
    "            quantity BIGINT, subtotal BIGINT>>, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country STRING,\n",
    "            row_time TIMESTAMP, processed_timestamp TIMESTAMP\"\"\"\n",
    "\n",
    "        orders_stream = spark.readStream.table(f\"{self.catalog}.{self.db_name}.silver_orders\")\n",
    "\n",
    "        customers_stream = (spark.readStream\n",
    "                                 .option(\"readChangeData\", True)\n",
    "                                 .option(\"startingVersion\", 2)\n",
    "                                 .table(f\"{self.catalog}.{self.db_name}.silver_customers\")\n",
    "                    )\n",
    "\n",
    "        stream_writer = (orders_stream.join(customers_stream, on=\"customer_id\")\n",
    "                                      .writeStream\n",
    "                                          .foreachBatch(data_upserter.upsert)\n",
    "                                          .option(\"checkpointLocation\", self.checkpoint_dir + \"/silver_customers_orders\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            stream_writer.trigger(processingTime=processing_time).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c57b38d-146a-4df7-bdef-5f4fb79d8fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SL = Silver('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecde7d8d-087d-4335-b258-d062221c9a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SL.upsert_customers(once=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5392b6-b151-4152-a915-73b8ef35a4bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('dev.bookstore_db.bronze')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd51493d-5b7c-4c43-9fd5-1ebebf15ca8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " df_country_lookup = spark.read.json(\"s3://bookstore-storage-001/raw/country_lookup\")\n",
    "\n",
    "schema = \"\"\"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING,\n",
    "            country_code STRING, row_status STRING, row_time timestamp\"\"\"\n",
    "\n",
    "df_stream = (spark.read\n",
    "                          .table(f\"dev.bookstore_db.bronze\")\n",
    "                          .filter(\"topic = 'customers'\")\n",
    "                          .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"c\"))\n",
    "                          .select(\"c.*\")\n",
    "                          #.join(F.broadcast(df_country_lookup), F.col(\"country_code\") == F.col(\"code\"))\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051ef469-d35a-4a9a-b4c4-a4445c2ece33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c2ee5e-eed1-4ff6-b7da-3f18ed3e5470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
