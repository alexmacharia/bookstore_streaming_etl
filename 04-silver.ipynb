{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b31af6-88e0-4b71-a03a-644161ad530c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0399fa2b-b044-490f-bef1-d70d42afdf15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter:\n",
    "    def __init__(self, merge_query, temp_view):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view = temp_view\n",
    "\n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        df_micro_batch.createOrReplaceTempView(self.temp_view)\n",
    "        df_microbatch.sparkSession.sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abcd0bd4-e624-420f-964c-79889f533d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "class CDCUpserter:\n",
    "    def __init__(self, merge_query, temp_view, id_col, sort_col):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view = temp_view\n",
    "        self.id_col = id_col\n",
    "        self.sort_col = sort_col\n",
    "\n",
    "    def upsert(self, df_micro_batch, batch_id):\n",
    "        window = Window.partition_by(self.id_col).orderBy(F.col(self.sort_col).desc())\n",
    "\n",
    "        df_micro_batch.filter(F.col(\"row_status\").isin([\"insert\", \"update\"])) \\\n",
    "            .withColumn(\"rank\", F.rank().over(window)) \\\n",
    "            .filter(\"rank == 1\") \\\n",
    "            .drop(\"rank\") \\\n",
    "            .createOrReplaceTempView(self.temp_view)\n",
    "\n",
    "        df_micro_batch.sparkSession.sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993e8ed3-7a97-4973-8cf6-458e78e09890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Silver:\n",
    "    def __init__(self, env):\n",
    "        self.Conf = Config()\n",
    "        self.checkpoint_dir = self.Conf.base_checkpoint_dir + \"/checkpoints\"\n",
    "        self.landing_dir = self.Conf.base_data_dir + \"/raw\"\n",
    "        self.catalog = env\n",
    "        self.db_name = self.Conf.db_name\n",
    "        self.maxFilesPerTrigger = self.Conf.maxFilesPerTrigger\n",
    "        spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "\n",
    "    \n",
    "    def upsert_customers(self, once=True, processing_time=\"10 seconds\"):\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.silver_customers c\n",
    "            USING ranked_updates r\n",
    "            ON c.customer_id=r.customer_id\n",
    "            WHEN MATCHED AND c.row_time < r.row_time\n",
    "              THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED\n",
    "              THEN INSERT *        \n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"ranked_updates\")\n",
    "\n",
    "        schema = \"\"\"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING,\n",
    "            country_code STRING, row_status STRING, row_time timestamp\"\"\"\n",
    "\n",
    "        df_country_lookup = spark.read.json(self.landing_dir + \"/country_lookup\")\n",
    "\n",
    "        df_stream = (spark.readStream\n",
    "                          .table(f\"{self.catalog}.{self.db_name}.bronze\")\n",
    "                          .fiter(\"topic = 'customers'\")\n",
    "                          .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"c\"))\n",
    "                          .select(\"c.*\")\n",
    "                          .join(F.broadcast(df_country_lookup), \"country_code\" == \"code\")\n",
    "        )\n",
    "\n",
    "        stream_writer = (df_stream.writeStream\n",
    "                                  .foreachBatch(data_upserter.upsert)\n",
    "                                  .option(\"checkpointLocation\", self.checkpoint_dir + \"/silver_customers\")                 \n",
    "                         )\n",
    "        \n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).strart()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
